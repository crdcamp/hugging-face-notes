{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcf24a3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "[Start here](https://huggingface.co/learn/mcp-course/en/unit1/key-concepts)\n",
    "\n",
    "# Key Concepts and Terminology\n",
    "\n",
    "MCP is often described as the \"USC=C for AI applications.\" Just as USB-C provides a standardized physical and logical interface for connecting various peripherals to computing devices, MCP offers a consistent protocol for linking AI models to external capabilities. This standardization benefits the entire ecosystem:\n",
    "* users enjoy simpler and more consistent experiences across AI applications\n",
    "* AI application developers gain easy integration with a growing ecosystem of tools and data sources\n",
    "* tool and data providers need only create a single implementation that works with multiple AI applications\n",
    "* the broader ecosystem benefits from increased interoperability, innovation, and reduced fragmentation\n",
    "\n",
    "## The Integration Problem\n",
    "\n",
    "The **MxN Integration Problem** refers to the challenge of connecting M different AI applications to N different external tools or data sources without a standardized approach.\n",
    "\n",
    "## Without MCP (M√óN Problem)\n",
    "\n",
    "Without a protocol like MCP, developers would need to create M√óN custom integrations‚Äîone for each possible pairing of an AI application with an external capability.\n",
    "\n",
    "Each AI application would need to integrate with each tool/data source individually. This is a very complex and expensive process which introduces a lot of friction for developers, and high maintenance costs.\n",
    "\n",
    "Once we have multiple models and multiple tools, the number of integrations becomes too large to manage, each with its own unique interface.\n",
    "\n",
    "## With MCP (M+N Solution)\n",
    "\n",
    "MCP transforms this into an M+N problem by providing a standard interface: each AI application implements the client side of MCP once, and each tool/data source implements the server side once. This dramatically reduces integration complexity and maintenance burden.\n",
    "\n",
    "## Core MCP Terminology\n",
    "\n",
    "MCP is a standard like HTTP or USB-C, and is a protocol for connecting AI applications to external tools and data sources. Therefore, using standard terminology is crucial to making the MCP work effectively.\n",
    "\n",
    "When documenting our applications and communicating with the community, we should use the following terminology.\n",
    "\n",
    "Just like client server relationships in HTTP, MCP has a client and a server.\n",
    "\n",
    "* **Host:** The user-facing AI application that end-users interact with directly. Examples include Anthropic‚Äôs Claude Desktop, AI-enhanced IDEs like Cursor, inference libraries like Hugging Face Python SDK, or custom applications built in libraries like LangChain or smolagents. Hosts initiate connections to MCP Servers and orchestrate the overall flow between user requests, LLM processing, and external tools.\n",
    "\n",
    "* **Client:** A component within the host application that manages communication with a specific MCP Server. Each Client maintains a 1:1 connection with a single Server, handling the protocol-level details of MCP communication and acting as an intermediary between the Host‚Äôs logic and the external Server.\n",
    "\n",
    "* **Server:** An external program or service that exposes capabilities (Tools, Resources, Prompts) via the MCP protocol.\n",
    "\n",
    "[Terminology](https://huggingface.co/learn/mcp-course/en/unit1/key-concepts#capabilities)\n",
    "\n",
    "**Then there's a bunch of other stuff you're better off just reading instead of typing along to.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28de52",
   "metadata": {},
   "source": [
    "# The Communication Protocol\n",
    "\n",
    "[Resource](https://huggingface.co/learn/mcp-course/en/unit1/communication-protocol)\n",
    "\n",
    "MCP defines a standardized communication protocol that enables Clients and Servers to exchange messages in a consistent, predictable way. This standardization is critical for interoperability across the community.\n",
    "\n",
    "## JSON-RPC: The Foundation\n",
    "\n",
    "At its core, MCP uses **JSON-RPC 2.0** as the message format for all communication between clients and servers. JSON-RPC is a lightweight remote procedure call protocol encoded in JSON, which makes it:\n",
    "* Human-readable and easy to debug\n",
    "* Language-agnostic, supporting implementation in any programming environment\n",
    "* Well-established, with clear specifications and widespread adoption\n",
    "\n",
    "The protocol defines three types of messages:\n",
    "\n",
    "### Requests\n",
    "\n",
    "Sent from Client to Server to initiate an operation. A request message includes:\n",
    "* A unique ID (`id`)\n",
    "* The method name to invoke (e.g. `tools/call`)\n",
    "* Parameters for the method (if any)\n",
    "\n",
    "Example request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e717144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 1,\n",
      "  \"params\": {\n",
      "    \"name\": \"weather\",\n",
      "    \"arguments\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "request = {\n",
    "    'jsonrpc': '2.0',\n",
    "    'id': 1,\n",
    "    'params' : {\n",
    "        'name': 'weather',\n",
    "        'arguments': {\n",
    "            'location': 'San Francisco'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "json_string = json.dumps(request)\n",
    "json_string_pretty = json.dumps(request, indent=2)\n",
    "print(json_string_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c0f3a",
   "metadata": {},
   "source": [
    "### Responses\n",
    "\n",
    "Sent from Server to Client in reply ot a Request. A Response message includes:\n",
    "* The same `id` as the corresponding Request\n",
    "* Either a `result` (for success) or and `error` (for failure)\n",
    "\n",
    "Example Success Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76eb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 1,\n",
      "  \"result\": {\n",
      "    \"temperature\": 62,\n",
      "    \"conditions\": \"Partly cloudy\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = {\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 1,\n",
    "  \"result\": {\n",
    "    \"temperature\": 62,\n",
    "    \"conditions\": \"Partly cloudy\"\n",
    "  }\n",
    "}\n",
    "\n",
    "json_string = json.dumps(response)\n",
    "json_string_pretty = json.dumps(response, indent=2)\n",
    "print(json_string_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd864",
   "metadata": {},
   "source": [
    "Example Error Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b91f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jsonrpc': '2.0',\n",
       " 'id': 1,\n",
       " 'error': {'code': -32602, 'message': 'Invalid location parameter'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 1,\n",
    "  \"error\": {\n",
    "    \"code\": -32602,\n",
    "    \"message\": \"Invalid location parameter\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18680a99",
   "metadata": {},
   "source": [
    "### Notifications\n",
    "One-way messages that don‚Äôt require a response. Typically sent from Server to Client to provide updates or notifications about events.\n",
    "\n",
    "Example Notification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ff824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jsonrpc': '2.0',\n",
       " 'method': 'progress',\n",
       " 'params': {'message': 'Processing data...', 'percent': 50}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"method\": \"progress\",\n",
    "  \"params\": {\n",
    "    \"message\": \"Processing data...\",\n",
    "    \"percent\": 50\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f0244",
   "metadata": {},
   "source": [
    "## Transport Mechanisms\n",
    "\n",
    "JSON-RPC defines the message format, but MCP also specifies how these messages are transported between Clients and Servers. Two primary transport mechanisms are supported.\n",
    "\n",
    "### stdio (Standard Input/Output)\n",
    "\n",
    "The stdio transport is used for local communication, where the Client and Server run on the same machine:\n",
    "\n",
    "The Host application launches the Server as a subprocess and communicates with it by writing to its standard input (stdin) and reading from its standard output (stdout).\n",
    "\n",
    "Use cases for this transport are local tools like file system access or running local scripts.\n",
    "\n",
    "The main Advantages of this transport are that it‚Äôs simple, no network configuration required, and securely sandboxed by the operating system.\n",
    "\n",
    "### HTTP + SSE (Server-Sent Events) / Streamable HTTP\n",
    "\n",
    "The HTTP+SSE transport is used for remote communication, where the Client and Server might be on different machines:\n",
    "\n",
    "Communication happens over HTTP, with the Server using Server-Sent Events (SSE) to push updates to the Client over a persistent connection.\n",
    "\n",
    "Use cases for this transport are connecting to remote APIs, cloud services, or shared resources.\n",
    "\n",
    "The main Advantages of this transport are that it works across networks, enables integration with web services, and is compatible with serverless environments.\n",
    "\n",
    "Recent updates to the MCP standard have introduced or refined ‚ÄúStreamable HTTP,‚Äù which offers more flexibility by allowing servers to dynamically upgrade to SSE for streaming when needed, while maintaining compatibility with serverless environments.\n",
    "\n",
    "Find notes on the **Interaction Lifecycle** [here](https://huggingface.co/learn/mcp-course/en/unit1/communication-protocol#the-interaction-lifecycle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4e5a4",
   "metadata": {},
   "source": [
    "# Understanding MCP Capabilities\n",
    "\n",
    "MCP Servers expose a variety of capabilities to Clients through the communication protocol. These capabilities fall into four main categories.\n",
    "\n",
    "## Tools\n",
    "\n",
    "Tools are executable functions or actions that the AI model can invoke through the MCP protocol\n",
    "* **Control:** Tools are typically **model-controlled**, meaning that the LLM decides when to call them based on the user's request and context.\n",
    "* **Safety:** Due to their ability to perform actions with side effects, tool execution can be dangerous. Therefore, they typically require explicit user approval.\n",
    "* **Use Cases:** Sending messages, creating tickets, querying APIs, performing calculations.\n",
    "\n",
    "**Example:** A weather tool that fetches current weather data for a given location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> dict:\n",
    "    \"\"\"Get the current weather for a specified location.\"\"\"\n",
    "    return {\n",
    "        \"temperature\": 72,\n",
    "        \"conditions\": \"Sunny\",\n",
    "        \"humidity\": 45\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74e9b2",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Resources provide read-only access to data sources, allowing the AI model to retrieve context without executing complex logic.\n",
    "* **Control:** Resources are **application-controlled**, meaning the Host application typically decides when to access them.\n",
    "* **Nature:** They are designed for data retrieval with minimal computation, similar to GET endpoints in REST APIs.\n",
    "* **Safety:** Since they are read-only, they typically present lower security risks than Tools.\n",
    "* **Use Cases:** Accessing file contents, retrieving database records, reading configuration information.\n",
    "\n",
    "**Example:** A resource that provides access to file contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b629114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read the contents of a file at the specified path.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7ea45",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Prompts are predefined templates or workflows that guide the interaction between the user, the AI model, and the Server's capabilities.\n",
    "* **Control:** Prompts are user-controlled, often presented as options in the Host application's UI.\n",
    "* **Purpose:** They structure interactions for optimal use of available Tools and Resources.\n",
    "* **Selection:** Users typically select a prompt before the AI model begins processing, setting context for the interaction.\n",
    "* **Use Cases:** Common workflows, specialized task templates, guided interactions.\n",
    "\n",
    "**Example:** A prompt template for generating a code review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576e19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_review(code: str, language: str) -> list:\n",
    "    \"\"\"Generate a code review for the provided code snippet.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a code reviewer examining {language} code. Provide a detailed review highlighting best practices, potential issues, and suggestions for improvement.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please review this {language} code:\\n\\n```{language}\\n{code}\\n```\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29ca84",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Sampling allows Servers to request the Client (specifically, the Host application) to perform LLM interactions.\n",
    "* **Control:** Sampling is server-initiated, but requires Client/Host facilitation.\n",
    "* **Purpose:** It enables server-driven agentic behaviors and potentially recursive or multi-step interactions.\n",
    "* **Safety:** Like Tools, sampling operations typically require user approval.\n",
    "* **Use Cases:**. Complex multi-step tasks, autonomous agent workflows, interactive processes.\n",
    "\n",
    "**Example:** A Server might request the Client to analyze data it has processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea1b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_sampling(messages, system_prompt=None, include_context=\"none\"):\n",
    "    \"\"\"Request LLM sampling from the client.\"\"\"\n",
    "    # In a real implementation, this would send a request to the client\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Analysis of the provided data...\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94738df7",
   "metadata": {},
   "source": [
    "The sampling flow follows these steps:\n",
    "1. Server sends a sampling/createMessage request to the client\n",
    "1. Client reviews the request and can modify it\n",
    "1. Client samples from an LLM\n",
    "1. Client reviews the completion\n",
    "1. Client returns the result to the server\n",
    "\n",
    "Refer to [this page](https://huggingface.co/learn/mcp-course/en/unit1/capabilities?resource-example=python&prompt-example=python#how-capabilities-work-together) for information on how these capabilities work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63306ad2",
   "metadata": {},
   "source": [
    "# Discovery Process\n",
    "\n",
    "One of MCP's key features is dynamic capability discovery. hen a Client connects to a server, it can query the available Tools, Resources, and Prompts through specific list methods:\n",
    "* `tools/list`: Discover available Tools\n",
    "* `resources/list`: Discover available Resources\n",
    "* `prompts/list`: Discover available Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc541967",
   "metadata": {},
   "source": [
    "# MCP SDK\n",
    "\n",
    "A Software Development Kit (SDK) helps software developers create applications for a specific platform, system, or programming language. Think of it kind of like a toolkit for app development. Typically, a basic SDK will include a compiler, debugger, and APIs.\n",
    "\n",
    "The MCP provides official SDKs for both JavaScript, Python and other languages. This makes it easy to implement MCP clients and servers in applications. These SDKs handle the low-level protocol details.\n",
    "\n",
    "## SDK Overview\n",
    "\n",
    "Both SDKs provide similar core functionality, following the MCP protocol specification we discussed earlier. They handle:\n",
    "* Protocol-level communication\n",
    "* Capability registration and discovery\n",
    "* Message serialization/deserialization\n",
    "* Connection management\n",
    "* Error handling\n",
    "\n",
    "## Core Primitives Implementation\n",
    "\n",
    "Now we're gonna explore how to implement each of the core primitives (Tools, Resources, and Prompts) using both SDKs.\n",
    "\n",
    "(Also, I'm going back to using double quotes. I like double quotes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "044a5e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Already running asyncio in this thread",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run the server\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mmcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/fastmcp/server.py:226\u001b[39m, in \u001b[36mFastMCP.run\u001b[39m\u001b[34m(self, transport, mount_path)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mmatch\u001b[39;00m transport:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         \u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_stdio_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msse\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    228\u001b[39m         anyio.run(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.run_sse_async(mount_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/anyio/_core/_eventloop.py:59\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(func, backend, backend_options, *args)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlready running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masynclib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in this thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     async_backend = get_async_backend(backend)\n",
      "\u001b[31mRuntimeError\u001b[39m: Already running asyncio in this thread"
     ]
    }
   ],
   "source": [
    "from mcp.server import FastMCP\n",
    "\n",
    "mcp = FastMCP('Weather Service')\n",
    "\n",
    "# Tool implementation\n",
    "@mcp.tool()\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a specified location.\"\"\"\n",
    "    return f\"Weather in {location}: Sunny, 72¬∞F\"\n",
    "\n",
    "# Resource implementation\n",
    "@mcp.resource(\"weather://{location}\")\n",
    "def weather_resource(location: str) -> str:\n",
    "    \"\"\"Provide weather data as a resource.\"\"\"\n",
    "    return f\"Weather data for {location}: Sunny, 72¬∞F\"\n",
    "\n",
    "# Prompt implementation\n",
    "@mcp.prompt()\n",
    "def weather_report(location: str) -> str:\n",
    "    \"\"\"Create a weather report prompt\"\"\"\n",
    "    return f\"\"\"You are a weather reporter. Weather report for {location}?\"\"\"\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00bbbe",
   "metadata": {},
   "source": [
    "Once you have your server implemented, you can start it running the server script.\n",
    "\n",
    "```mcp dev server.py```\n",
    "\n",
    "This will initialize a development server running the file `sever.py`. And log the following output:\n",
    "\n",
    "```Starting MCP inspector...\n",
    "‚öôÔ∏è Proxy server listening on port 6277\n",
    "Spawned stdio transport\n",
    "Connected MCP client to backing server transport\n",
    "Created web app transport\n",
    "Set up MCP proxy\n",
    "üîç MCP Inspector is up and running at http://127.0.0.1:6274 üöÄ```\n",
    "\n",
    "You can then open the MCP Inspector at http://127.0.0.1:6274 to see the server's capabilities and interact with them. This will bring you to a beautiful UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c0a4c",
   "metadata": {},
   "source": [
    "# MCP Clients\n",
    "\n",
    "MCP CLients are crucial components that act as the bridge between AI applications (Hosts) and external capabilities provided by MCP Servers. Think of the Host as your main application (like an AI assistant or IDE) and the Client as a specialized module within that Host responsible for handling MCP communications.\n",
    "\n",
    "# User Interface Client\n",
    "\n",
    "## Chat Interface Clients\n",
    "* Claude Desktop (Anthropic)\n",
    "\n",
    "## Interactive Development Clients\n",
    "* VS Code extensions with MCP\n",
    "* Cursor IDE\n",
    "* Zed editor\n",
    "\n",
    "These clients support connecting to multiple MCP servers and real-time tool invocation.\n",
    "\n",
    "## Configuring MCP Clients\n",
    "\n",
    "Effective deployment of MCP servers and clients requires proper configuration. The MCP specification is still evolving, so the configuration methods are subject to evolution. We'll focus on the current best practices for configuration.\n",
    "\n",
    "**MCP Configuration Files**\n",
    "\n",
    "MCP hosts use configuration files to manage server connections. These files define which servers are available and how to connect to them.\n",
    "\n",
    "Fortunately, the configuration files are very simple, easy to understand, and consistent across major MCP hosts.\n",
    "\n",
    "**mcp.json Structure**\n",
    "\n",
    "The standard configuration file for MCP is named `mcp.json`. Here's the basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"Server Name\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"stdio|sse\",\n",
    "        # Transport-specific configuration\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ca3d5",
   "metadata": {},
   "source": [
    "In this example, we have a single server with a name and a transport type. The transport type is either `studio` or `sse`.\n",
    "\n",
    "**Configuration for stdio Transport**\n",
    "\n",
    "For local servers using stdio transport, the configuration includes the command and arguments to launch the server process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"File Explorer\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"stdio\",\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"/path/to/file_explorer_server.py\"] // This is an example, we'll use a real server in the next unit\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a843122",
   "metadata": {},
   "source": [
    "Here, we have a server called \"File Explorer\" that is a local script.\n",
    "\n",
    "**Configuration for HTTP+SSE Transport**\n",
    "\n",
    "For remote servers using HTTP+SSE transport, the configuration includes the server URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"Remote API Server\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"sse\",\n",
    "        \"url\": \"https://example.com/mcp-server\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dde1f1",
   "metadata": {},
   "source": [
    "**Environment Variables Configuration**\n",
    "\n",
    "In Python, we use the `os` module to access environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Access environment variables\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "if not github_token:\n",
    "    raise ValueError(\"GITHUB_TOKEN environment variable is required\")\n",
    "\n",
    "# Use the token in your server code\n",
    "def make_github_request():\n",
    "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
    "    # ... rest of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94fdb7",
   "metadata": {},
   "source": [
    "The corresponding configuration in `mcp.json` would look like this:\n",
    "\n",
    "```{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"GitHub API\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"stdio\",\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"/path/to/github_server.py\"], // This is an example, we'll use a real server in the next unit\n",
    "        \"env\": {\n",
    "          \"GITHUB_TOKEN\": \"your_github_token\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910f16c",
   "metadata": {},
   "source": [
    "Go to [this link](https://huggingface.co/learn/mcp-course/en/unit1/mcp-clients#configuration-examples) for configuration examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3c62c",
   "metadata": {},
   "source": [
    "# Tiny Agents Clients\n",
    "\n",
    "Let's explore how to use MCP Clients within code.\n",
    "\n",
    "You can also use tiny agents as MCP Clients to connect directly to MCP servers from your code. Tiny agents provide a simple way to create AI agents that can use tools from MCP servers.\n",
    "\n",
    "Tiny Agent can run MCP servers with a command line environment.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we will need to install `npx` with the following command:\n",
    "\n",
    "```npm install -g npx```\n",
    "\n",
    "Then, we will need to install the huggingface_hub package with the MCP support. This will allow us to run MCP servers and clients.\n",
    "\n",
    "```pip install \"huggingface_hub[mcp]>=0.32.0\"```\n",
    "\n",
    "Then, we will need to log in to the Hugging Face Hub to access the MCP servers. You can do this with the `huggingface-cli` command line tool. You will need a [login token](https://huggingface.co/docs/huggingface_hub/v0.32.3/en/quick-start#authentication) to do this.\n",
    "\n",
    "```huggingface-cli login```\n",
    "\n",
    "**Configure Access Token Permissions**\n",
    "\n",
    "After creating your Hugging Face access token and logging in, you need to ensure your token has the proper permissions to work with inference providers.\n",
    "\n",
    "1. Go to your [Hugging Face Access Tokens page](https://huggingface.co/settings/tokens)\n",
    "1. Find your MCP token and click the three dots (‚ãÆ) next to it\n",
    "1. Select ‚ÄúEdit permissions‚Äù\n",
    "1. Under the Inference section, check the box for:\n",
    "* ‚ÄúMake calls to Inference Providers‚Äù\n",
    "1. Save your changes\n",
    "\n",
    "This permission is required because tiny agents need to make API calls to hosted models like `Qwen/Qwen2.5-72B-Instruct` through providers like Nebius.\n",
    "\n",
    "## Connecting to MCP Servers\n",
    "\n",
    "Now, let's create an agent congfiguration file `agent.json`. You can find this file somewhere else (figure it out you fool).\n",
    "\n",
    "Now you can run the agent:\n",
    "\n",
    "```tiny-agents run agent.json```\n",
    "\n",
    "In the [this video](), we run the agent and ask it to open a new tab in the browser.\n",
    "\n",
    "(Seems they messed up with the documentation. I can't find the video anywhere.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7134e",
   "metadata": {},
   "source": [
    "# Hugging Face MCP Server\n",
    "\n",
    "The Hugging Face MCP Server connects your MCP-compatible AI assistant (for example VS Code, Cursor, Zed, or Claude Desktop) directly to the Hugging Face Hub. Once connected, your assistant can search and explore Hub resources and use community tools, all from within your editor, chat, or CLI.\n",
    "\n",
    "**What you can do**\n",
    "\n",
    "* Search and explore Hub resources: models, datasets, Spaces, and papers.\n",
    "* Run community tools via MCP‚Äëcompatible Gradio apps hosted on Spaces.\n",
    "* Bring results back into your assistant with metadata, links, and context.\n",
    "\n",
    "**Built-in tools**\n",
    "\n",
    "The server provides curated tools that work across supported clients:\n",
    "\n",
    "* Models search and exploration (filter by task, library, downloads, likes)\n",
    "* Datasets search and exploration (filter by tags, size, modality)\n",
    "* Spaces semantic search (find apps by capability, e.g., TTS, ASR, OCR)\n",
    "* Papers semantic search (discover relevant research on the Hub)\n",
    "\n",
    "**Get started**\n",
    "\n",
    "1. Open your MCP settings: visit https://huggingface.co/settings/mcp while logged in.\n",
    "1. Pick your client: select your MCP‚Äëcompatible client (for example VS Code, Cursor, Zed, Claude Desktop). The page shows client‚Äëspecific instructions and a ready‚Äëto‚Äëcopy configuration snippet.\n",
    "1. Paste and restart: copy the snippet into your client‚Äôs MCP configuration, save, and restart/reload the client. You should see ‚ÄúHugging Face‚Äù (or similar) listed as a connected MCP server in your client.\n",
    "\n",
    "**Using the server**\n",
    "\n",
    "After connecting, ask your assistance to use the Hugging Face tools.\n",
    "\n",
    "Your assistant will call MCP tools exposed by the Hugging Face MCP Server. You can then open the resource on the Hub or continue iterating in the same chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83289a",
   "metadata": {},
   "source": [
    "# Gradio MCP Integration\n",
    "\n",
    "Gradio is a popular Python library for quickly creating customizable web interfaces for machine learning models.\n",
    "\n",
    "## Introduction to Gradio\n",
    "\n",
    "Gradio allows developers to create UIs for their models with just a few lines of Python code. It‚Äôs particularly useful for:\n",
    "* Creating demos and prototypes\n",
    "* Sharing models with non-technical users\n",
    "* Testing and debugging model behavior\n",
    "\n",
    "With the addition of MCP support, Gradio now offers a straightforward way to expose AI model capabilities through the standardized MCP protocol.\n",
    "\n",
    "Combining Gradio with MCP allows you to create both human-friendly interfaces and AI-accessible tools with minimal code. But best of all, Gradio is already well-used by the AI community, so you can use it to share your MCP Servers with others.\n",
    "\n",
    "You‚Äôll also need an LLM application that supports tool calling using the MCP protocol, such as Cursor ( known as ‚ÄúMCP Hosts‚Äù).\n",
    "\n",
    "# Creating an MCP Server with Gradio\n",
    "\n",
    "Let‚Äôs walk through a basic example of creating an MCP Server using Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759443bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/10/25 09:56:54] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> StreamableHTTP session manager started                  <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py#112\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/10/25 09:56:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m StreamableHTTP session manager started                  \u001b]8;id=943945;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=707403;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py#112\u001b\\\u001b[2m112\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860/gradio_api/startup-events</span>      <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttp://127.0.0.1:7860/gradio_api/startup-events\u001b[0m      \u001b]8;id=697446;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=973854;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">HEAD</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860/</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>            <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mHEAD\u001b[0m \u001b[4;94mhttp://127.0.0.1:7860/\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m            \u001b]8;id=386494;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=558025;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "üî® Launching MCP server:\n",
      "** Streamable HTTP URL: http://127.0.0.1:7860/gradio_api/mcp/\n",
      "* [Deprecated] SSE URL: http://127.0.0.1:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.gradio.app/pkg-version</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span> <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttps://api.gradio.app/pkg-version\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m \u001b]8;id=402021;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=993739;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def letter_counter(word: str, letter: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of occurrences of a letter in a word or text.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input text to search through\n",
    "        letter (str): The letter to search for\n",
    "\n",
    "    Returns:\n",
    "        int: The number of times the letter appears in the text\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    letter = letter.lower()\n",
    "    count = word.count(letter)\n",
    "    return count\n",
    "\n",
    "# Create a standard Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=letter_counter,\n",
    "    inputs=[\"textbox\", \"textbox\"],\n",
    "    outputs=\"number\",\n",
    "    title=\"Letter Counter\",\n",
    "    description=\"Enter text and a letter to count how many times the letter appears in the text.\"\n",
    ")\n",
    "\n",
    "# Launch both the Gradio web interface and the MCP server\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(mcp_server=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819e0ed",
   "metadata": {},
   "source": [
    "## How it works behind the scenes\n",
    "\n",
    "When you set `mcp_server=True` in `launch()`, several things happen:\n",
    "1. Gradio functions are automatically converted to MCP Tools\n",
    "1. Input components determine the response format\n",
    "1. The Gradio server now also listens for MCP protocol messages\n",
    "1. JSON-RPC over HTTP+SSE is set up for client-server communication\n",
    "\n",
    "## Key Features of the Gradio MCP Integration\n",
    "1. **Tool Conversion:** Each API endpoint in your gradio app is automatically converted into an MCP tool with a corresponding name, description, and input schema. To view the tools and schemas, visit http://your-server:port/gradio_api/mcp/schema or go to the ‚ÄúView API‚Äù link in the footer of your Gradio app, and then click on ‚ÄúMCP‚Äù.\n",
    "1. **Environment Variable Support:** There are two ways to enable the MCP server functionality:\n",
    "* Using the `mcp_server` parameter in `launch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842c2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/10/25 09:57:00] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">HEAD</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860/</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>            <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/10/25 09:57:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mHEAD\u001b[0m \u001b[4;94mhttp://127.0.0.1:7860/\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m            \u001b]8;id=570206;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=183112;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "üî® Launching MCP server:\n",
      "** Streamable HTTP URL: http://127.0.0.1:7860/gradio_api/mcp/\n",
      "* [Deprecated] SSE URL: http://127.0.0.1:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(mcp_server=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f330f6",
   "metadata": {},
   "source": [
    "* Using environment variables:\n",
    "\n",
    "```export GRADIO_MCP_SERVER=True```\n",
    "\n",
    "3. **File Handling:** The server automatically handles file data conversions, including:\n",
    "* Conversing base64-encoded strings to file data\n",
    "* Processing image files and returning them in the correct format\n",
    "* Managing temporary file storage\n",
    "\n",
    "It is **strongly** recommended that input images and files be passed as full URLs as MCP Clients do not always handle local files correctly.\n",
    "\n",
    "4. Hosted MCP Servers on ü§ó Spaces: You can publish your Gradio application for free on Hugging Face Spaces, which will allow you to have a free hosted MCP server. Here‚Äôs an example of such a Space: https://huggingface.co/spaces/abidlabs/mcp-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecc5ff",
   "metadata": {},
   "source": [
    "# Building and End-to-End MCP Application\n",
    "\n",
    "In this unit, we‚Äôll build a complete MCP application from scratch, focusing on creating a server with Gradio and connecting it with multiple clients. This hands-on approach will give you practical experience with the entire MCP ecosystem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
