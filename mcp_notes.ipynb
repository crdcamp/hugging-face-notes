{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bcf24a3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "[Start here](https://huggingface.co/learn/mcp-course/en/unit1/key-concepts)\n",
    "\n",
    "# Key Concepts and Terminology\n",
    "\n",
    "MCP is often described as the \"USC=C for AI applications.\" Just as USB-C provides a standardized physical and logical interface for connecting various peripherals to computing devices, MCP offers a consistent protocol for linking AI models to external capabilities. This standardization benefits the entire ecosystem:\n",
    "* users enjoy simpler and more consistent experiences across AI applications\n",
    "* AI application developers gain easy integration with a growing ecosystem of tools and data sources\n",
    "* tool and data providers need only create a single implementation that works with multiple AI applications\n",
    "* the broader ecosystem benefits from increased interoperability, innovation, and reduced fragmentation\n",
    "\n",
    "## The Integration Problem\n",
    "\n",
    "The **MxN Integration Problem** refers to the challenge of connecting M different AI applications to N different external tools or data sources without a standardized approach.\n",
    "\n",
    "## Without MCP (M√óN Problem)\n",
    "\n",
    "Without a protocol like MCP, developers would need to create M√óN custom integrations‚Äîone for each possible pairing of an AI application with an external capability.\n",
    "\n",
    "Each AI application would need to integrate with each tool/data source individually. This is a very complex and expensive process which introduces a lot of friction for developers, and high maintenance costs.\n",
    "\n",
    "Once we have multiple models and multiple tools, the number of integrations becomes too large to manage, each with its own unique interface.\n",
    "\n",
    "## With MCP (M+N Solution)\n",
    "\n",
    "MCP transforms this into an M+N problem by providing a standard interface: each AI application implements the client side of MCP once, and each tool/data source implements the server side once. This dramatically reduces integration complexity and maintenance burden.\n",
    "\n",
    "## Core MCP Terminology\n",
    "\n",
    "MCP is a standard like HTTP or USB-C, and is a protocol for connecting AI applications to external tools and data sources. Therefore, using standard terminology is crucial to making the MCP work effectively.\n",
    "\n",
    "When documenting our applications and communicating with the community, we should use the following terminology.\n",
    "\n",
    "Just like client server relationships in HTTP, MCP has a client and a server.\n",
    "\n",
    "* **Host:** The user-facing AI application that end-users interact with directly. Examples include Anthropic‚Äôs Claude Desktop, AI-enhanced IDEs like Cursor, inference libraries like Hugging Face Python SDK, or custom applications built in libraries like LangChain or smolagents. Hosts initiate connections to MCP Servers and orchestrate the overall flow between user requests, LLM processing, and external tools.\n",
    "\n",
    "* **Client:** A component within the host application that manages communication with a specific MCP Server. Each Client maintains a 1:1 connection with a single Server, handling the protocol-level details of MCP communication and acting as an intermediary between the Host‚Äôs logic and the external Server.\n",
    "\n",
    "* **Server:** An external program or service that exposes capabilities (Tools, Resources, Prompts) via the MCP protocol.\n",
    "\n",
    "[Terminology](https://huggingface.co/learn/mcp-course/en/unit1/key-concepts#capabilities)\n",
    "\n",
    "**Then there's a bunch of other stuff you're better off just reading instead of typing along to.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28de52",
   "metadata": {},
   "source": [
    "# The Communication Protocol\n",
    "\n",
    "[Resource](https://huggingface.co/learn/mcp-course/en/unit1/communication-protocol)\n",
    "\n",
    "MCP defines a standardized communication protocol that enables Clients and Servers to exchange messages in a consistent, predictable way. This standardization is critical for interoperability across the community.\n",
    "\n",
    "## JSON-RPC: The Foundation\n",
    "\n",
    "At its core, MCP uses **JSON-RPC 2.0** as the message format for all communication between clients and servers. JSON-RPC is a lightweight remote procedure call protocol encoded in JSON, which makes it:\n",
    "* Human-readable and easy to debug\n",
    "* Language-agnostic, supporting implementation in any programming environment\n",
    "* Well-established, with clear specifications and widespread adoption\n",
    "\n",
    "The protocol defines three types of messages:\n",
    "\n",
    "### Requests\n",
    "\n",
    "Sent from Client to Server to initiate an operation. A request message includes:\n",
    "* A unique ID (`id`)\n",
    "* The method name to invoke (e.g. `tools/call`)\n",
    "* Parameters for the method (if any)\n",
    "\n",
    "Example request:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e717144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 1,\n",
      "  \"params\": {\n",
      "    \"name\": \"weather\",\n",
      "    \"arguments\": {\n",
      "      \"location\": \"San Francisco\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "request = {\n",
    "    'jsonrpc': '2.0',\n",
    "    'id': 1,\n",
    "    'params' : {\n",
    "        'name': 'weather',\n",
    "        'arguments': {\n",
    "            'location': 'San Francisco'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "json_string = json.dumps(request)\n",
    "json_string_pretty = json.dumps(request, indent=2)\n",
    "print(json_string_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5c0f3a",
   "metadata": {},
   "source": [
    "### Responses\n",
    "\n",
    "Sent from Server to Client in reply ot a Request. A Response message includes:\n",
    "* The same `id` as the corresponding Request\n",
    "* Either a `result` (for success) or and `error` (for failure)\n",
    "\n",
    "Example Success Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c76eb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": 1,\n",
      "  \"result\": {\n",
      "    \"temperature\": 62,\n",
      "    \"conditions\": \"Partly cloudy\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = {\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 1,\n",
    "  \"result\": {\n",
    "    \"temperature\": 62,\n",
    "    \"conditions\": \"Partly cloudy\"\n",
    "  }\n",
    "}\n",
    "\n",
    "json_string = json.dumps(response)\n",
    "json_string_pretty = json.dumps(response, indent=2)\n",
    "print(json_string_pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbd864",
   "metadata": {},
   "source": [
    "Example Error Response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5b91f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jsonrpc': '2.0',\n",
       " 'id': 1,\n",
       " 'error': {'code': -32602, 'message': 'Invalid location parameter'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"id\": 1,\n",
    "  \"error\": {\n",
    "    \"code\": -32602,\n",
    "    \"message\": \"Invalid location parameter\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18680a99",
   "metadata": {},
   "source": [
    "### Notifications\n",
    "One-way messages that don‚Äôt require a response. Typically sent from Server to Client to provide updates or notifications about events.\n",
    "\n",
    "Example Notification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ff824a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jsonrpc': '2.0',\n",
       " 'method': 'progress',\n",
       " 'params': {'message': 'Processing data...', 'percent': 50}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"jsonrpc\": \"2.0\",\n",
    "  \"method\": \"progress\",\n",
    "  \"params\": {\n",
    "    \"message\": \"Processing data...\",\n",
    "    \"percent\": 50\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f0244",
   "metadata": {},
   "source": [
    "## Transport Mechanisms\n",
    "\n",
    "JSON-RPC defines the message format, but MCP also specifies how these messages are transported between Clients and Servers. Two primary transport mechanisms are supported.\n",
    "\n",
    "### stdio (Standard Input/Output)\n",
    "\n",
    "The stdio transport is used for local communication, where the Client and Server run on the same machine:\n",
    "\n",
    "The Host application launches the Server as a subprocess and communicates with it by writing to its standard input (stdin) and reading from its standard output (stdout).\n",
    "\n",
    "Use cases for this transport are local tools like file system access or running local scripts.\n",
    "\n",
    "The main Advantages of this transport are that it‚Äôs simple, no network configuration required, and securely sandboxed by the operating system.\n",
    "\n",
    "### HTTP + SSE (Server-Sent Events) / Streamable HTTP\n",
    "\n",
    "The HTTP+SSE transport is used for remote communication, where the Client and Server might be on different machines:\n",
    "\n",
    "Communication happens over HTTP, with the Server using Server-Sent Events (SSE) to push updates to the Client over a persistent connection.\n",
    "\n",
    "Use cases for this transport are connecting to remote APIs, cloud services, or shared resources.\n",
    "\n",
    "The main Advantages of this transport are that it works across networks, enables integration with web services, and is compatible with serverless environments.\n",
    "\n",
    "Recent updates to the MCP standard have introduced or refined ‚ÄúStreamable HTTP,‚Äù which offers more flexibility by allowing servers to dynamically upgrade to SSE for streaming when needed, while maintaining compatibility with serverless environments.\n",
    "\n",
    "Find notes on the **Interaction Lifecycle** [here](https://huggingface.co/learn/mcp-course/en/unit1/communication-protocol#the-interaction-lifecycle)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f4e5a4",
   "metadata": {},
   "source": [
    "# Understanding MCP Capabilities\n",
    "\n",
    "MCP Servers expose a variety of capabilities to Clients through the communication protocol. These capabilities fall into four main categories.\n",
    "\n",
    "## Tools\n",
    "\n",
    "Tools are executable functions or actions that the AI model can invoke through the MCP protocol\n",
    "* **Control:** Tools are typically **model-controlled**, meaning that the LLM decides when to call them based on the user's request and context.\n",
    "* **Safety:** Due to their ability to perform actions with side effects, tool execution can be dangerous. Therefore, they typically require explicit user approval.\n",
    "* **Use Cases:** Sending messages, creating tickets, querying APIs, performing calculations.\n",
    "\n",
    "**Example:** A weather tool that fetches current weather data for a given location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b04cb025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> dict:\n",
    "    \"\"\"Get the current weather for a specified location.\"\"\"\n",
    "    return {\n",
    "        \"temperature\": 72,\n",
    "        \"conditions\": \"Sunny\",\n",
    "        \"humidity\": 45\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec74e9b2",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Resources provide read-only access to data sources, allowing the AI model to retrieve context without executing complex logic.\n",
    "* **Control:** Resources are **application-controlled**, meaning the Host application typically decides when to access them.\n",
    "* **Nature:** They are designed for data retrieval with minimal computation, similar to GET endpoints in REST APIs.\n",
    "* **Safety:** Since they are read-only, they typically present lower security risks than Tools.\n",
    "* **Use Cases:** Accessing file contents, retrieving database records, reading configuration information.\n",
    "\n",
    "**Example:** A resource that provides access to file contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b629114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n",
    "    \"\"\"Read the contents of a file at the specified path.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c7ea45",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "Prompts are predefined templates or workflows that guide the interaction between the user, the AI model, and the Server's capabilities.\n",
    "* **Control:** Prompts are user-controlled, often presented as options in the Host application's UI.\n",
    "* **Purpose:** They structure interactions for optimal use of available Tools and Resources.\n",
    "* **Selection:** Users typically select a prompt before the AI model begins processing, setting context for the interaction.\n",
    "* **Use Cases:** Common workflows, specialized task templates, guided interactions.\n",
    "\n",
    "**Example:** A prompt template for generating a code review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "576e19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code_review(code: str, language: str) -> list:\n",
    "    \"\"\"Generate a code review for the provided code snippet.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a code reviewer examining {language} code. Provide a detailed review highlighting best practices, potential issues, and suggestions for improvement.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please review this {language} code:\\n\\n```{language}\\n{code}\\n```\"\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d29ca84",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Sampling allows Servers to request the Client (specifically, the Host application) to perform LLM interactions.\n",
    "* **Control:** Sampling is server-initiated, but requires Client/Host facilitation.\n",
    "* **Purpose:** It enables server-driven agentic behaviors and potentially recursive or multi-step interactions.\n",
    "* **Safety:** Like Tools, sampling operations typically require user approval.\n",
    "* **Use Cases:**. Complex multi-step tasks, autonomous agent workflows, interactive processes.\n",
    "\n",
    "**Example:** A Server might request the Client to analyze data it has processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea1b855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_sampling(messages, system_prompt=None, include_context=\"none\"):\n",
    "    \"\"\"Request LLM sampling from the client.\"\"\"\n",
    "    # In a real implementation, this would send a request to the client\n",
    "    return {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Analysis of the provided data...\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94738df7",
   "metadata": {},
   "source": [
    "The sampling flow follows these steps:\n",
    "1. Server sends a sampling/createMessage request to the client\n",
    "1. Client reviews the request and can modify it\n",
    "1. Client samples from an LLM\n",
    "1. Client reviews the completion\n",
    "1. Client returns the result to the server\n",
    "\n",
    "Refer to [this page](https://huggingface.co/learn/mcp-course/en/unit1/capabilities?resource-example=python&prompt-example=python#how-capabilities-work-together) for information on how these capabilities work together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63306ad2",
   "metadata": {},
   "source": [
    "# Discovery Process\n",
    "\n",
    "One of MCP's key features is dynamic capability discovery. hen a Client connects to a server, it can query the available Tools, Resources, and Prompts through specific list methods:\n",
    "* `tools/list`: Discover available Tools\n",
    "* `resources/list`: Discover available Resources\n",
    "* `prompts/list`: Discover available Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc541967",
   "metadata": {},
   "source": [
    "# MCP SDK\n",
    "\n",
    "A Software Development Kit (SDK) helps software developers create applications for a specific platform, system, or programming language. Think of it kind of like a toolkit for app development. Typically, a basic SDK will include a compiler, debugger, and APIs.\n",
    "\n",
    "The MCP provides official SDKs for both JavaScript, Python and other languages. This makes it easy to implement MCP clients and servers in applications. These SDKs handle the low-level protocol details.\n",
    "\n",
    "## SDK Overview\n",
    "\n",
    "Both SDKs provide similar core functionality, following the MCP protocol specification we discussed earlier. They handle:\n",
    "* Protocol-level communication\n",
    "* Capability registration and discovery\n",
    "* Message serialization/deserialization\n",
    "* Connection management\n",
    "* Error handling\n",
    "\n",
    "## Core Primitives Implementation\n",
    "\n",
    "Now we're gonna explore how to implement each of the core primitives (Tools, Resources, and Prompts) using both SDKs.\n",
    "\n",
    "(Also, I'm going back to using double quotes. I like double quotes.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "044a5e7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Already running asyncio in this thread",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run the server\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[43mmcp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/fastmcp/server.py:226\u001b[39m, in \u001b[36mFastMCP.run\u001b[39m\u001b[34m(self, transport, mount_path)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mmatch\u001b[39;00m transport:\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstdio\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m         \u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_stdio_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28;01mcase\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33msse\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    228\u001b[39m         anyio.run(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m.run_sse_async(mount_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/anyio/_core/_eventloop.py:59\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(func, backend, backend_options, *args)\u001b[39m\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAlready running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00masynclib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in this thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     async_backend = get_async_backend(backend)\n",
      "\u001b[31mRuntimeError\u001b[39m: Already running asyncio in this thread"
     ]
    }
   ],
   "source": [
    "from mcp.server import FastMCP\n",
    "\n",
    "mcp = FastMCP('Weather Service')\n",
    "\n",
    "# Tool implementation\n",
    "@mcp.tool()\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get the current weather for a specified location.\"\"\"\n",
    "    return f\"Weather in {location}: Sunny, 72¬∞F\"\n",
    "\n",
    "# Resource implementation\n",
    "@mcp.resource(\"weather://{location}\")\n",
    "def weather_resource(location: str) -> str:\n",
    "    \"\"\"Provide weather data as a resource.\"\"\"\n",
    "    return f\"Weather data for {location}: Sunny, 72¬∞F\"\n",
    "\n",
    "# Prompt implementation\n",
    "@mcp.prompt()\n",
    "def weather_report(location: str) -> str:\n",
    "    \"\"\"Create a weather report prompt\"\"\"\n",
    "    return f\"\"\"You are a weather reporter. Weather report for {location}?\"\"\"\n",
    "\n",
    "# Run the server\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f00bbbe",
   "metadata": {},
   "source": [
    "Once you have your server implemented, you can start it running the server script.\n",
    "\n",
    "```mcp dev server.py```\n",
    "\n",
    "This will initialize a development server running the file `sever.py`. And log the following output:\n",
    "\n",
    "```Starting MCP inspector...\n",
    "‚öôÔ∏è Proxy server listening on port 6277\n",
    "Spawned stdio transport\n",
    "Connected MCP client to backing server transport\n",
    "Created web app transport\n",
    "Set up MCP proxy\n",
    "üîç MCP Inspector is up and running at http://127.0.0.1:6274 üöÄ```\n",
    "\n",
    "You can then open the MCP Inspector at http://127.0.0.1:6274 to see the server's capabilities and interact with them. This will bring you to a beautiful UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c0a4c",
   "metadata": {},
   "source": [
    "# MCP Clients\n",
    "\n",
    "MCP CLients are crucial components that act as the bridge between AI applications (Hosts) and external capabilities provided by MCP Servers. Think of the Host as your main application (like an AI assistant or IDE) and the Client as a specialized module within that Host responsible for handling MCP communications.\n",
    "\n",
    "# User Interface Client\n",
    "\n",
    "## Chat Interface Clients\n",
    "* Claude Desktop (Anthropic)\n",
    "\n",
    "## Interactive Development Clients\n",
    "* VS Code extensions with MCP\n",
    "* Cursor IDE\n",
    "* Zed editor\n",
    "\n",
    "These clients support connecting to multiple MCP servers and real-time tool invocation.\n",
    "\n",
    "## Configuring MCP Clients\n",
    "\n",
    "Effective deployment of MCP servers and clients requires proper configuration. The MCP specification is still evolving, so the configuration methods are subject to evolution. We'll focus on the current best practices for configuration.\n",
    "\n",
    "**MCP Configuration Files**\n",
    "\n",
    "MCP hosts use configuration files to manage server connections. These files define which servers are available and how to connect to them.\n",
    "\n",
    "Fortunately, the configuration files are very simple, easy to understand, and consistent across major MCP hosts.\n",
    "\n",
    "**mcp.json Structure**\n",
    "\n",
    "The standard configuration file for MCP is named `mcp.json`. Here's the basic structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f2f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"Server Name\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"stdio|sse\",\n",
    "        # Transport-specific configuration\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901ca3d5",
   "metadata": {},
   "source": [
    "In this example, we have a single server with a name and a transport type. The transport type is either `studio` or `sse`.\n",
    "\n",
    "**Configuration for stdio Transport**\n",
    "\n",
    "For local servers using stdio transport, the configuration includes the command and arguments to launch the server process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"File Explorer\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"stdio\",\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"/path/to/file_explorer_server.py\"] // This is an example, we'll use a real server in the next unit\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a843122",
   "metadata": {},
   "source": [
    "Here, we have a server called \"File Explorer\" that is a local script.\n",
    "\n",
    "**Configuration for HTTP+SSE Transport**\n",
    "\n",
    "For remote servers using HTTP+SSE transport, the configuration includes the server URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"Remote API Server\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"sse\",\n",
    "        \"url\": \"https://example.com/mcp-server\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dde1f1",
   "metadata": {},
   "source": [
    "**Environment Variables Configuration**\n",
    "\n",
    "In Python, we use the `os` module to access environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9139ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Access environment variables\n",
    "github_token = os.environ.get(\"GITHUB_TOKEN\")\n",
    "if not github_token:\n",
    "    raise ValueError(\"GITHUB_TOKEN environment variable is required\")\n",
    "\n",
    "# Use the token in your server code\n",
    "def make_github_request():\n",
    "    headers = {\"Authorization\": f\"Bearer {github_token}\"}\n",
    "    # ... rest of your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f94fdb7",
   "metadata": {},
   "source": [
    "The corresponding configuration in `mcp.json` would look like this:\n",
    "\n",
    "```{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"GitHub API\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"stdio\",\n",
    "        \"command\": \"python\",\n",
    "        \"args\": [\"/path/to/github_server.py\"], // This is an example, we'll use a real server in the next unit\n",
    "        \"env\": {\n",
    "          \"GITHUB_TOKEN\": \"your_github_token\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910f16c",
   "metadata": {},
   "source": [
    "Go to [this link](https://huggingface.co/learn/mcp-course/en/unit1/mcp-clients#configuration-examples) for configuration examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3c62c",
   "metadata": {},
   "source": [
    "# Tiny Agents Clients\n",
    "\n",
    "Let's explore how to use MCP Clients within code.\n",
    "\n",
    "You can also use tiny agents as MCP Clients to connect directly to MCP servers from your code. Tiny agents provide a simple way to create AI agents that can use tools from MCP servers.\n",
    "\n",
    "Tiny Agent can run MCP servers with a command line environment.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we will need to install `npx` with the following command:\n",
    "\n",
    "```npm install -g npx```\n",
    "\n",
    "Then, we will need to install the huggingface_hub package with the MCP support. This will allow us to run MCP servers and clients.\n",
    "\n",
    "```pip install \"huggingface_hub[mcp]>=0.32.0\"```\n",
    "\n",
    "Then, we will need to log in to the Hugging Face Hub to access the MCP servers. You can do this with the `huggingface-cli` command line tool. You will need a [login token](https://huggingface.co/docs/huggingface_hub/v0.32.3/en/quick-start#authentication) to do this.\n",
    "\n",
    "```huggingface-cli login```\n",
    "\n",
    "**Configure Access Token Permissions**\n",
    "\n",
    "After creating your Hugging Face access token and logging in, you need to ensure your token has the proper permissions to work with inference providers.\n",
    "\n",
    "1. Go to your [Hugging Face Access Tokens page](https://huggingface.co/settings/tokens)\n",
    "1. Find your MCP token and click the three dots (‚ãÆ) next to it\n",
    "1. Select ‚ÄúEdit permissions‚Äù\n",
    "1. Under the Inference section, check the box for:\n",
    "* ‚ÄúMake calls to Inference Providers‚Äù\n",
    "1. Save your changes\n",
    "\n",
    "This permission is required because tiny agents need to make API calls to hosted models like `Qwen/Qwen2.5-72B-Instruct` through providers like Nebius.\n",
    "\n",
    "## Connecting to MCP Servers\n",
    "\n",
    "Now, let's create an agent congfiguration file `agent.json`. You can find this file somewhere else (figure it out you fool).\n",
    "\n",
    "Now you can run the agent:\n",
    "\n",
    "```tiny-agents run agent.json```\n",
    "\n",
    "In the [this video](), we run the agent and ask it to open a new tab in the browser.\n",
    "\n",
    "(Seems they messed up with the documentation. I can't find the video anywhere.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a7134e",
   "metadata": {},
   "source": [
    "# Hugging Face MCP Server\n",
    "\n",
    "The Hugging Face MCP Server connects your MCP-compatible AI assistant (for example VS Code, Cursor, Zed, or Claude Desktop) directly to the Hugging Face Hub. Once connected, your assistant can search and explore Hub resources and use community tools, all from within your editor, chat, or CLI.\n",
    "\n",
    "**What you can do**\n",
    "\n",
    "* Search and explore Hub resources: models, datasets, Spaces, and papers.\n",
    "* Run community tools via MCP‚Äëcompatible Gradio apps hosted on Spaces.\n",
    "* Bring results back into your assistant with metadata, links, and context.\n",
    "\n",
    "**Built-in tools**\n",
    "\n",
    "The server provides curated tools that work across supported clients:\n",
    "\n",
    "* Models search and exploration (filter by task, library, downloads, likes)\n",
    "* Datasets search and exploration (filter by tags, size, modality)\n",
    "* Spaces semantic search (find apps by capability, e.g., TTS, ASR, OCR)\n",
    "* Papers semantic search (discover relevant research on the Hub)\n",
    "\n",
    "**Get started**\n",
    "\n",
    "1. Open your MCP settings: visit https://huggingface.co/settings/mcp while logged in.\n",
    "1. Pick your client: select your MCP‚Äëcompatible client (for example VS Code, Cursor, Zed, Claude Desktop). The page shows client‚Äëspecific instructions and a ready‚Äëto‚Äëcopy configuration snippet.\n",
    "1. Paste and restart: copy the snippet into your client‚Äôs MCP configuration, save, and restart/reload the client. You should see ‚ÄúHugging Face‚Äù (or similar) listed as a connected MCP server in your client.\n",
    "\n",
    "**Using the server**\n",
    "\n",
    "After connecting, ask your assistance to use the Hugging Face tools.\n",
    "\n",
    "Your assistant will call MCP tools exposed by the Hugging Face MCP Server. You can then open the resource on the Hub or continue iterating in the same chat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83289a",
   "metadata": {},
   "source": [
    "# Gradio MCP Integration\n",
    "\n",
    "Gradio is a popular Python library for quickly creating customizable web interfaces for machine learning models.\n",
    "\n",
    "## Introduction to Gradio\n",
    "\n",
    "Gradio allows developers to create UIs for their models with just a few lines of Python code. It‚Äôs particularly useful for:\n",
    "* Creating demos and prototypes\n",
    "* Sharing models with non-technical users\n",
    "* Testing and debugging model behavior\n",
    "\n",
    "With the addition of MCP support, Gradio now offers a straightforward way to expose AI model capabilities through the standardized MCP protocol.\n",
    "\n",
    "Combining Gradio with MCP allows you to create both human-friendly interfaces and AI-accessible tools with minimal code. But best of all, Gradio is already well-used by the AI community, so you can use it to share your MCP Servers with others.\n",
    "\n",
    "You‚Äôll also need an LLM application that supports tool calling using the MCP protocol, such as Cursor ( known as ‚ÄúMCP Hosts‚Äù).\n",
    "\n",
    "# Creating an MCP Server with Gradio\n",
    "\n",
    "Let‚Äôs walk through a basic example of creating an MCP Server using Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759443bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/10/25 09:56:54] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> StreamableHTTP session manager started                  <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py#112\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/10/25 09:56:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m StreamableHTTP session manager started                  \u001b]8;id=943945;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=707403;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/mcp/server/streamable_http_manager.py#112\u001b\\\u001b[2m112\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860/gradio_api/startup-events</span>      <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttp://127.0.0.1:7860/gradio_api/startup-events\u001b[0m      \u001b]8;id=697446;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=973854;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">HEAD</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860/</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>            <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mHEAD\u001b[0m \u001b[4;94mhttp://127.0.0.1:7860/\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m            \u001b]8;id=386494;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=558025;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "üî® Launching MCP server:\n",
      "** Streamable HTTP URL: http://127.0.0.1:7860/gradio_api/mcp/\n",
      "* [Deprecated] SSE URL: http://127.0.0.1:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.gradio.app/pkg-version</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span> <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m \u001b[4;94mhttps://api.gradio.app/pkg-version\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m \u001b]8;id=402021;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=993739;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def letter_counter(word: str, letter: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of occurrences of a letter in a word or text.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input text to search through\n",
    "        letter (str): The letter to search for\n",
    "\n",
    "    Returns:\n",
    "        int: The number of times the letter appears in the text\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    letter = letter.lower()\n",
    "    count = word.count(letter)\n",
    "    return count\n",
    "\n",
    "# Create a standard Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=letter_counter,\n",
    "    inputs=[\"textbox\", \"textbox\"],\n",
    "    outputs=\"number\",\n",
    "    title=\"Letter Counter\",\n",
    "    description=\"Enter text and a letter to count how many times the letter appears in the text.\"\n",
    ")\n",
    "\n",
    "# Launch both the Gradio web interface and the MCP server\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(mcp_server=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6819e0ed",
   "metadata": {},
   "source": [
    "## How it works behind the scenes\n",
    "\n",
    "When you set `mcp_server=True` in `launch()`, several things happen:\n",
    "1. Gradio functions are automatically converted to MCP Tools\n",
    "1. Input components determine the response format\n",
    "1. The Gradio server now also listens for MCP protocol messages\n",
    "1. JSON-RPC over HTTP+SSE is set up for client-server communication\n",
    "\n",
    "## Key Features of the Gradio MCP Integration\n",
    "1. **Tool Conversion:** Each API endpoint in your gradio app is automatically converted into an MCP tool with a corresponding name, description, and input schema. To view the tools and schemas, visit http://your-server:port/gradio_api/mcp/schema or go to the ‚ÄúView API‚Äù link in the footer of your Gradio app, and then click on ‚ÄúMCP‚Äù.\n",
    "1. **Environment Variable Support:** There are two ways to enable the MCP server functionality:\n",
    "* Using the `mcp_server` parameter in `launch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842c2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[11/10/25 09:57:00] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">HEAD</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:7860/</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>            <a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[11/10/25 09:57:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mHEAD\u001b[0m \u001b[4;94mhttp://127.0.0.1:7860/\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m            \u001b]8;id=570206;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=183112;file:///Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/httpx/_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "üî® Launching MCP server:\n",
      "** Streamable HTTP URL: http://127.0.0.1:7860/gradio_api/mcp/\n",
      "* [Deprecated] SSE URL: http://127.0.0.1:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.launch(mcp_server=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f330f6",
   "metadata": {},
   "source": [
    "* Using environment variables:\n",
    "\n",
    "```export GRADIO_MCP_SERVER=True```\n",
    "\n",
    "3. **File Handling:** The server automatically handles file data conversions, including:\n",
    "* Conversing base64-encoded strings to file data\n",
    "* Processing image files and returning them in the correct format\n",
    "* Managing temporary file storage\n",
    "\n",
    "It is **strongly** recommended that input images and files be passed as full URLs as MCP Clients do not always handle local files correctly.\n",
    "\n",
    "4. Hosted MCP Servers on ü§ó Spaces: You can publish your Gradio application for free on Hugging Face Spaces, which will allow you to have a free hosted MCP server. Here‚Äôs an example of such a Space: https://huggingface.co/spaces/abidlabs/mcp-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ecc5ff",
   "metadata": {},
   "source": [
    "# Building and End-to-End MCP Application\n",
    "\n",
    "In this unit, we‚Äôll build a complete MCP application from scratch, focusing on creating a server with Gradio and connecting it with multiple clients. This hands-on approach will give you practical experience with the entire MCP ecosystem.\n",
    "\n",
    "## Our End-to-End Project\n",
    "\n",
    "We'll build a sentiment analysis application that consists of three main parts: the server, the client, and the deployment.\n",
    "\n",
    "![Alt image](images/e2e_project.png)\n",
    "\n",
    "**Server Side**\n",
    "* Uses Gradio to create a web interface and MCP server via `gr.Interface`\n",
    "* Implements a sentiment analysis tool using TextBlob\n",
    "* Exposes the tool through both HTTP and MCP protocols\n",
    "\n",
    "**Client Side**\n",
    "* Implements a HuggingFace.js client\n",
    "* Or, creates a smolagents Python client\n",
    "* Demonstrates how to use the same server with different client implementations\n",
    "\n",
    "**Deployment**\n",
    "* Deploys the server to Hugging Face Spaces\n",
    "* Configures the clients to work with the deployed server\n",
    "\n",
    "Alright, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1cef8",
   "metadata": {},
   "source": [
    "# Building the Gradio MCP Server\n",
    "\n",
    "In this section, we‚Äôll create our sentiment analysis MCP server using Gradio. This server will expose a sentiment analysis tool that can be used by both human users through a web interface and AI models through the MCP protocol.\n",
    "\n",
    "## Introduction to Gradio MCP Integration\n",
    "\n",
    "Gradio provides a straightforward way to create MCP servers by automatically converting your Python functions into MCP tools. When you set `mcp_server=True` in `launch()`, Gradio:\n",
    "\n",
    "1. Automatically converts your functions into MCP Tools\n",
    "1. Maps input components to tool argument schemas\n",
    "1. Determines response formats from output components\n",
    "1. Sets up JSON-RPC over HTTP+SSE for client-server communication\n",
    "1. Creates both a web interface and an MCP server endpoint\n",
    "\n",
    "## Setting Up the Project\n",
    "\n",
    "# Creating the Server\n",
    "\n",
    "**Hugging face spaces needs an app.py file to build the space. So the name of the python file has to be app.py**\n",
    "\n",
    "Create a new file called `app.py` with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47303219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "\n",
      "üî® Launching MCP server:\n",
      "** Streamable HTTP URL: http://127.0.0.1:7860/gradio_api/mcp/\n",
      "* [Deprecated] SSE URL: http://127.0.0.1:7860/gradio_api/mcp/sse\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import gradio as gr\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_analysis(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to analyze\n",
    "    \n",
    "    Returns:\n",
    "    str: A JSON string containing polarity, subjectivity, and assessment\n",
    "    \"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment\n",
    "\n",
    "    result ={\n",
    "        \"polarity\": round(sentiment.polarity, 2), # -1 (negative) to 1 (positive)\n",
    "        \"subjectivity\": round(sentiment.subjectivity, 2), # 0 (objective) to 1 (subjective)\n",
    "        \"assessment\": \"positive\" if sentiment.polarity > 0 else \"negative\" if sentiment.polarity < 0 else \"neutral\"\n",
    "    }\n",
    "\n",
    "    return json.dumps(result)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=sentiment_analysis,\n",
    "    inputs=gr.Textbox(placeholder=\"Enter text to analyze...\"),\n",
    "    outputs=gr.Textbox(),\n",
    "    title=\"Text Sentiment Analysis\",\n",
    "    description=\"Analyze the sentiment of text using TextBlob\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(mcp_server=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37d20b2",
   "metadata": {},
   "source": [
    "There's then a section that explains the code, but it's pretty straightforward. I'll just read over it for now to see if there's anything surprising happening here.\n",
    "\n",
    "Now start the server by running:\n",
    "\n",
    "```python app.py```\n",
    "\n",
    "You should see output indicating that both the web interface and MCP server are running. The web interface will be available at http://localhost:7860, and the MCP server at http://localhost:7860/gradio_api/mcp/sse.\n",
    "\n",
    "## Testing the Server\n",
    "\n",
    "You can test the server in two ways:\n",
    "\n",
    "1. **Web Interface:** \n",
    "* Open `http://localhost:7860` in your browser\n",
    "* Enter some text and hit \"Submit\"\n",
    "* Poof there ya go.\n",
    "\n",
    "1. **MCP Schema:**\n",
    "* Visit `http://localhost:7860/gradio_api/mcp/schema`\n",
    "* This shows the MCP tool schema that clients will use\n",
    "* You can also find this in the \"View API\" link in the footer of your Gradio app\n",
    "\n",
    "## Troubleshooting Tips\n",
    "\n",
    "1. Type Hints and Docstrings:\n",
    "* Always provide type hints for your function parameters and return values\n",
    "* Include a docstring with an ‚ÄúArgs:‚Äù block for each parameter\n",
    "* This helps Gradio generate accurate MCP tool schemas\n",
    "\n",
    "2. String Inputs:\n",
    "* When in doubt, accept input arguments as str\n",
    "* Convert them to the desired type inside the function\n",
    "* This provides better compatibility with MCP clients\n",
    "\n",
    "3. SSE Support:\n",
    "* Some MCP clients don‚Äôt support SSE-based MCP Servers\n",
    "* In those cases, use mcp-remote\n",
    "\n",
    "## SSE Support:\n",
    "* Some MCP clients don't support SSE-based MCP Servers\n",
    "* In those cases, use `mcp-remote`:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"gradio\": {\n",
    "      \"command\": \"npx\",\n",
    "      \"args\": [\n",
    "        \"mcp-remote\",\n",
    "        \"http://localhost:7860/gradio_api/mcp/sse\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Deploying to Hugging Face Spaces\n",
    "\n",
    "1. Create a new Space on Hugging Face:\n",
    "* Go to huggingface.co/spaces\n",
    "* Click ‚ÄúNew Space‚Äù\n",
    "* Name your space (e.g., ‚Äúmcp-sentiment‚Äù)\n",
    "* Choose ‚ÄúGradio‚Äù as the SDK\n",
    "* Click ‚ÄúCreate Space‚Äù\n",
    "\n",
    "2. Create a `requirements.txt` file:\n",
    "\n",
    "```\n",
    "gradio[mcp]\n",
    "textblob\n",
    "```\n",
    "\n",
    "3. Push your code to the Space:\n",
    "\n",
    "```\n",
    "git init\n",
    "git add app.py requirements.txt\n",
    "git commit -m \"Initial commit\"\n",
    "git remote add origin https://huggingface.co/spaces/YOUR_USERNAME/mcp-sentiment\n",
    "git push -u origin main\n",
    "```\n",
    "\n",
    "Your MCP server will now be available at:\n",
    "\n",
    "`https://YOUR_USERNAME-mcp-sentiment.hf.space/gradio_api/mcp/sse`\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that we have our MCP server running, we'll create clients to interact with it. Let's move on to building our first client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0f1b2",
   "metadata": {},
   "source": [
    "# Building MCP Clients\n",
    "\n",
    "In this section, we‚Äôll create clients that can interact with our MCP server using different programming languages. We‚Äôll implement both a JavaScript client using HuggingFace.js and a Python client using smolagents.\n",
    "\n",
    "**Configuring MCP Clients**\n",
    "\n",
    "Effective deployment of MCP servers and clients requires proper configuration. The MCP specification is still evolving, so the configuration methods are subject to evolution. We‚Äôll focus on the current best practices for configuration.\n",
    "\n",
    "**MCP Configuration Files**\n",
    "\n",
    "MCP hosts use configuration files to manage server connections. These files define which servers are available and how to connect to them.\n",
    "\n",
    "The configuration files are very simple, easy to understand, and consistent across major MCP hosts.\n",
    "\n",
    "**mcp.json Structure**\n",
    "\n",
    "The standard configuration file for MCP is named mcp.json. Here‚Äôs the basic structure:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"MCP Server\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"sse\",\n",
    "        \"url\": \"http://localhost:7860/gradio_api/mcp/sse\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "In this example, we have a single server configured to use SSE transport, connecting to a local Gradio server running on port 7860.\n",
    "\n",
    "We‚Äôve connected to the Gradio app via SSE transport because we assume that the gradio app is running on a remote server. However, if you want to connect to a local script, `stdio` transport instead of `sse` transport is a better option.\n",
    "\n",
    "## Configuration for HTTP+SSE Transport\n",
    "\n",
    "For remote servers using HTTP+SSE transport, the configuration includes the server URL:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"name\": \"Remote MCP Server\",\n",
    "      \"transport\": {\n",
    "        \"type\": \"sse\",\n",
    "        \"url\": \"https://example.com/gradio_api/mcp/sse\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration allows your UI client to communicate with the Gradio MCP server using the MCP protocol, enabling seamless integration between your frontend and the MCP service.\n",
    "\n",
    "## Configuring a UI MCP Client\n",
    "\n",
    "When working with Gradio MCP servers, you can configure your UI client to connect to the server using the MCP protocol. Here's how to set it up:\n",
    "\n",
    "### Basic Configuration\n",
    "\n",
    "Create a new file called `config.json` with the following configuration:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"mcp\": {\n",
    "      \"url\": \"http://localhost:7860/gradio_api/mcp/sse\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Configuring an MCP Client within Cursor IDE\n",
    "\n",
    "I don't need this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a84b5d",
   "metadata": {},
   "source": [
    "# Using MCP with Local and Open Source Models\n",
    "\n",
    "In this section, we'll connect MCP with local and open-source models using Continue, a tool for building AI coding assistants that works with local tools like Ollama.\n",
    "\n",
    "## Setup Continue\n",
    "\n",
    "You can install Continue from the VS Code marketplace. Find it [here](https://marketplace.visualstudio.com/items?itemName=Continue.continue).\n",
    "\n",
    "Look [here](https://huggingface.co/learn/mcp-course/en/unit2/continue-client#local-models) for how to run local models with this.\n",
    "\n",
    "## Local Models\n",
    "\n",
    "There are many ways to run local models that are compatible with Continue. Three populat options are Ollama, Llama.cpp, and LM Studio. You already know what Ollama is, Llama.cpp is a high-performance C+= library for running LLMs that also includes an OpenAI-compatible server. LM studio you're also already familiar with.\n",
    "\n",
    "You can access local models from the Hugging Face Hub and get commands and quick links for all major local inference apps.\n",
    "\n",
    "**It is important that wee use models that have tool calling as a built-in feature**, i.e. Codestral Qwen and Llama 3.1x.\n",
    "1. Create a folder called `.continue/models` at the top level of your workspace\n",
    "1. Add a file to this folder to configure your model provider. For example, `local-models.yaml`.\n",
    "1. cAdd the following configuration (this ones's for Ollama specifically)\n",
    "\n",
    "```\n",
    "name: Ollama Devstral model\n",
    "version: 0.0.1\n",
    "schema: v1\n",
    "models:\n",
    "  - provider: ollama\n",
    "    model: unsloth/devstral-small-2505-gguf:Q4_K_M\n",
    "    defaultCompletionOptions:\n",
    "      contextLength: 8192\n",
    "    name: Ollama Devstral-Small\n",
    "    roles:\n",
    "      - chat\n",
    "      - edit\n",
    "```\n",
    "\n",
    "By default, each model has a max context length, in this case it is 128000 tokens. This setup includes a larger use of that context window to perform multiple MCP requests and needs to be able to handle more tokens.\n",
    "\n",
    "Refer to [here](https://docs.continue.dev/reference#models) in case you start getting fancy with this shit.\n",
    "\n",
    "Also, just for fun, here's how we can run a local model with Llama.cpp. This model came from [here](https://huggingface.co/unsloth/Devstral-Small-2505-GGUF). In fact, this is the only way you should be running these models! Why save them to your laptop when they can be containerized (sorta) in a virtual environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3a8512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "GatedRepoError",
     "evalue": "403 Client Error. (Request ID: Root=1-69138abd-59bc41880d7450fe14496554;c1cb79fe-44f3-4812-aa9a-14ad465977f0)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf.\nAccess to model google/gemma-7b is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-7b to ask for access.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:402\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mGatedRepoError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# !pip install llama-cpp-python\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m llm = \u001b[43mLlama\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma-7b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m\t\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgemma-7b.gguf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/llama_cpp/llama.py:2309\u001b[39m, in \u001b[36mLlama.from_pretrained\u001b[39m\u001b[34m(cls, repo_id, filename, additional_files, local_dir, local_dir_use_symlinks, cache_dir, **kwargs)\u001b[39m\n\u001b[32m   2306\u001b[39m filename = Path(matching_file).name\n\u001b[32m   2308\u001b[39m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2309\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2310\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2312\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_dir_use_symlinks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2316\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m additional_files:\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m additonal_file_name \u001b[38;5;129;01min\u001b[39;00m additional_files:\n\u001b[32m   2320\u001b[39m         \u001b[38;5;66;03m# find the additional shard file:\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:1007\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    987\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    988\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    989\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1004\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1005\u001b[39m     )\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:1114\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1111\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[32m   1113\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1114\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n\u001b[32m   1117\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33metag must have been retrieved from server\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:1655\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1647\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m hf.co look-ups and downloads online, set \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlocal_files_only\u001b[39m\u001b[33m'\u001b[39m\u001b[33m to False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1649\u001b[39m     )\n\u001b[32m   1650\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1651\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1652\u001b[39m ):\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1655\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1657\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[32m   1658\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[32m   1659\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1660\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1661\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m is on.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1662\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhead_call_error\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:1543\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1541\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1542\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1543\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1546\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1547\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1548\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:1460\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[39m\n\u001b[32m   1457\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1459\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1462\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1465\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1466\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1467\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1468\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1469\u001b[39m hf_raise_for_status(r)\n\u001b[32m   1471\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:283\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m300\u001b[39m <= response.status_code <= \u001b[32m399\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/file_download.py:307\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[32m    306\u001b[39m response = http_backoff(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:419\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_code == \u001b[33m\"\u001b[39m\u001b[33mGatedRepo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    416\u001b[39m     message = (\n\u001b[32m    417\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    418\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(GatedRepoError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m error_message == \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    422\u001b[39m     message = (\n\u001b[32m    423\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    424\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    427\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[33mAccess to this resource is disabled.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    428\u001b[39m     )\n",
      "\u001b[31mGatedRepoError\u001b[39m: 403 Client Error. (Request ID: Root=1-69138abd-59bc41880d7450fe14496554;c1cb79fe-44f3-4812-aa9a-14ad465977f0)\n\nCannot access gated repo for url https://huggingface.co/google/gemma-7b/resolve/main/gemma-7b.gguf.\nAccess to model google/gemma-7b is restricted and you are not in the authorized list. Visit https://huggingface.co/google/gemma-7b to ask for access."
     ]
    }
   ],
   "source": [
    "# !pip install llama-cpp-python\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"google/gemma-7b\",\n",
    "\tfilename=\"gemma-7b.gguf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2ec0de",
   "metadata": {},
   "source": [
    "Needed to get permission for this, and I don't feel like going through the authentication process. So, let's try a transformers pipeline instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db57127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christiancamp/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3b17bd61a74b1abdd5dd89cdc544de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m      6\u001b[39m messages = [\n\u001b[32m      7\u001b[39m     {\n\u001b[32m      8\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m     },\n\u001b[32m     14\u001b[39m ]\n\u001b[32m     15\u001b[39m inputs = processor.apply_chat_template(\n\u001b[32m     16\u001b[39m \tmessages,\n\u001b[32m     17\u001b[39m \tadd_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \treturn_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m ).to(model.device)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(processor.decode(outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1344\u001b[39m, in \u001b[36mQwen3VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1314\u001b[39m \u001b[38;5;129m@check_model_inputs\u001b[39m\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1330\u001b[39m ) -> Union[\u001b[38;5;28mtuple\u001b[39m, Qwen3VLCausalLMOutputWithPast]:\n\u001b[32m   1331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1332\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1333\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1342\u001b[39m \u001b[33;03m        TODO: Add example\u001b[39;00m\n\u001b[32m   1343\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m     hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1360\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1138\u001b[39m, in \u001b[36mQwen3VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m video_mask = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m     image_embeds, deepstack_image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     image_embeds = torch.cat(image_embeds, dim=\u001b[32m0\u001b[39m).to(inputs_embeds.device, inputs_embeds.dtype)\n\u001b[32m   1140\u001b[39m     image_mask, _ = \u001b[38;5;28mself\u001b[39m.get_placeholder_mask(\n\u001b[32m   1141\u001b[39m         input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n\u001b[32m   1142\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1061\u001b[39m, in \u001b[36mQwen3VLModel.get_image_features\u001b[39m\u001b[34m(self, pixel_values, image_grid_thw)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1052\u001b[39m \u001b[33;03mEncodes images into continuous embeddings that can be forwarded to the language model. The deepstack visual features are also returned.\u001b[39;00m\n\u001b[32m   1053\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1058\u001b[39m \u001b[33;03m        The temporal, height and width of feature shape of each image in LLM.\u001b[39;00m\n\u001b[32m   1059\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1060\u001b[39m pixel_values = pixel_values.type(\u001b[38;5;28mself\u001b[39m.visual.dtype)\n\u001b[32m-> \u001b[39m\u001b[32m1061\u001b[39m image_embeds, deepstack_image_embeds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m split_sizes = (image_grid_thw.prod(-\u001b[32m1\u001b[39m) // \u001b[38;5;28mself\u001b[39m.visual.spatial_merge_size**\u001b[32m2\u001b[39m).tolist()\n\u001b[32m   1063\u001b[39m image_embeds = torch.split(image_embeds, split_sizes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:739\u001b[39m, in \u001b[36mQwen3VLVisionModel.forward\u001b[39m\u001b[34m(self, hidden_states, grid_thw, **kwargs)\u001b[39m\n\u001b[32m    737\u001b[39m deepstack_feature_lists = []\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_num, blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.blocks):\n\u001b[32m--> \u001b[39m\u001b[32m739\u001b[39m     hidden_states = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    745\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m layer_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.deepstack_visual_indexes:\n\u001b[32m    746\u001b[39m         deepstack_feature = \u001b[38;5;28mself\u001b[39m.deepstack_merger_list[\u001b[38;5;28mself\u001b[39m.deepstack_visual_indexes.index(layer_num)](\n\u001b[32m    747\u001b[39m             hidden_states\n\u001b[32m    748\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:267\u001b[39m, in \u001b[36mQwen3VLVisionBlock.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    261\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    265\u001b[39m     **kwargs,\n\u001b[32m    266\u001b[39m ) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrotary_pos_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m     hidden_states = hidden_states + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.norm2(hidden_states))\n\u001b[32m    275\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:231\u001b[39m, in \u001b[36mQwen3VLVisionAttention.forward\u001b[39m\u001b[34m(self, hidden_states, cu_seqlens, rotary_pos_emb, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     lengths = cu_seqlens[\u001b[32m1\u001b[39m:] - cu_seqlens[:-\u001b[32m1\u001b[39m]\n\u001b[32m    226\u001b[39m     splits = [\n\u001b[32m    227\u001b[39m         torch.split(tensor, lengths.tolist(), dim=\u001b[32m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m (query_states, key_states, value_states)\n\u001b[32m    228\u001b[39m     ]\n\u001b[32m    230\u001b[39m     attn_outputs = [\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m         \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m            \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m            \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m q, k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(*splits)\n\u001b[32m    243\u001b[39m     ]\n\u001b[32m    244\u001b[39m     attn_output = torch.cat(attn_outputs, dim=\u001b[32m1\u001b[39m)\n\u001b[32m    246\u001b[39m attn_output = attn_output.reshape(seq_length, -\u001b[32m1\u001b[39m).contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hugging-face-notes/hugging-env/lib/python3.13/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"Qwen/Qwen3-VL-4B-Instruct\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0269db6",
   "metadata": {},
   "source": [
    "Okay! Enough fun! Let's get back to it...\n",
    "\n",
    "### The tool handshake\n",
    "\n",
    "Tools provide a powerful way for models to interface with the external world. They are provided to the model as a JSON object with a name and an arguments schema. For example, a `read_file` tool with a `filepath` argument will give the model the ability to request the contents of a specific file.\n",
    "\n",
    "![Alt image](images/tool_handshake.png)\n",
    "\n",
    "The following handshake describes how the Agent uses tools:\n",
    "\n",
    "1. In Agent mode, available tools are sent along with user chat requests\n",
    "1. The model can choose to include a tool call in its response\n",
    "1. The user gives permission. This step is skipped if the policy for that tool is set to Automatic\n",
    "1. Continue calls the tool using built-in functionality or the MCP server that offers that particular tool\n",
    "1. Continue sends the result back to the model\n",
    "1. The model responds, potentially with another tool call, and step 2 begins again\n",
    "\n",
    "Continue supports multiple local model providers. You can use different models for different tasks or switch models as needed. This section focuses on local-first solutions, but Continue does work with the dreaded API calls as well.\n",
    "\n",
    "## Local Model Integration with MCP\n",
    "\n",
    "Now that we have everything set up, let's add an existing MCP server. Below is a quick example of setting up a new MCP server for use in your assistant:\n",
    "1. Create a folder called `.continue/mcpServers` at the top level of your workspace.\n",
    "1. Add a file called playwright-mcp.yaml to this folder\n",
    "1. Write the following contents to playwright-mcp.yaml and save\n",
    "\n",
    "Now test your MCP server by prompting the following command:\n",
    "\n",
    "1. Using playwright, navigate to https://news.ycombinator.com.\n",
    "\n",
    "2. Extract the titles and URLs of the top 4 posts on the homepage.\n",
    "\n",
    "3. Create a file named hn.txt in the root directory of the project.\n",
    "\n",
    "4. Save this list as plain text in the hn.txt file, with each line containing the title and URL separated by a hyphen.\n",
    "\n",
    "Do not output code or instructions‚Äîjust complete the task and confirm when it is done.\n",
    "\n",
    "I'm not sure exactly where they want me to type this command, but I don't think this is even exactly what I'm looking for in the context of local models. I'll definitely need to do some storage cleanup after this though my goodness.\n",
    "\n",
    "For the sake of time constraints (as much as I'd love to keep exploring here), it's time to move on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e07eb",
   "metadata": {},
   "source": [
    "# Gradio as an MCP Client\n",
    "\n",
    "In the previous section, we explored how to create an MCP Server using Gradio and connect to it using an MCP Client. In this section, we‚Äôre going to explore how to use Gradio as an MCP Client to connect to an MCP Server.\n",
    "\n",
    "We‚Äôll connect to an MCP server similar to the one we created in the previous section but with additional features, and use it to answer questions.\n",
    "\n",
    "## MCP Client in Gradio\n",
    "\n",
    "### Connect to an example MCP Server\n",
    "\n",
    "Let‚Äôs connect to an example MCP Server that is already running on Hugging Face. We‚Äôll use [this one](https://huggingface.co/spaces/abidlabs/mcp-tool-http) for this example. It‚Äôs a space that contains a collection of MCP tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79a25c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/f7m4q3pj7j3f6tby_sb7n5280000gn/T/ipykernel_28835/929892553.py:3: FutureWarning: Parameter 'structured_output' was not specified. Currently it defaults to False, but in version 1.25, the default will change to True. To suppress this warning, explicitly set structured_output=True (new behavior) or structured_output=False (legacy behavior). See documentation at https://huggingface.co/docs/smolagents/tutorials/tools#structured-output-and-output-schema-support for more details.\n",
      "  with MCPClient(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mcp_tool_http_prime_factors: Compute the prime factorization of a positive integer.\n",
      "mcp_tool_http_generate_cheetah_image: Generate a cheetah image. Returns: The generated cheetah image.\n",
      "mcp_tool_http_image_orientation: Returns whether image is portrait or landscape.\n",
      "mcp_tool_http_sepia: Apply a sepia filter to the input image.\n"
     ]
    }
   ],
   "source": [
    "from smolagents.mcp_client import MCPClient\n",
    "\n",
    "with MCPClient(\n",
    "    {\"url\": \"https://abidlabs-mcp-tool-http.hf.space/gradio_api/mcp/sse\", \"transport\": \"sse\",}\n",
    ") as tools:\n",
    "    # Tools from the remote server are available\n",
    "    print(\"\\n\".join(f\"{t.name}: {t.description}\" for t in tools))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b1a28a",
   "metadata": {},
   "source": [
    "### Connect to the MCP Server from Gradio\n",
    "\n",
    "Great, now that you‚Äôve connected to an example MCP Server. Now, you need to use it in an example application.\n",
    "\n",
    "First, we need to install the smolagents, Gradio and mcp-client libraries, if we haven‚Äôt already:\n",
    "\n",
    "```pip install \"smolagents[mcp]\" \"gradio[mcp]\" mcp fastmcp```\n",
    "\n",
    "Now, we can import the necessary libraries and create a simple Gradio interface that uses the MCP Client to connect to the MCP Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab377bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "\n",
    "from mcp import StdioServerParameters\n",
    "from smolagents import InferenceClientModel, CodeAgent, ToolCollection, MCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5c1e52",
   "metadata": {},
   "source": [
    "Next, we‚Äôll connect to the MCP Server and get the tools that we can use to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73561324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/f7m4q3pj7j3f6tby_sb7n5280000gn/T/ipykernel_33880/2021446007.py:1: FutureWarning: Parameter 'structured_output' was not specified. Currently it defaults to False, but in version 1.25, the default will change to True. To suppress this warning, explicitly set structured_output=True (new behavior) or structured_output=False (legacy behavior). See documentation at https://huggingface.co/docs/smolagents/tutorials/tools#structured-output-and-output-schema-support for more details.\n",
      "  mcp_client = MCPClient(\n"
     ]
    }
   ],
   "source": [
    "mcp_client = MCPClient(\n",
    "    {\"url\": \"https://abidlabs-mcp-tool-http.hf.space/gradio_api/mcp/sse\", \"transport\": \"sse\",} # This is the MCP Client we created in the previous section\n",
    ")\n",
    "tools = mcp_client.get_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cfd2cf",
   "metadata": {},
   "source": [
    "Now that we have the tools, we can create a simple agent that uses them to answer questions. We‚Äôll just use a simple `InferenceClientModel` and the default model from `smolagents` for now.\n",
    "\n",
    "It is important to pass your api_key to the `InferenceClientModel`. You can access the token from your huggingface account. [check here](https://huggingface.co/docs/hub/en/security-tokens)., and set the access token with the environment variable HF_TOKEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "573ed7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model = InferenceClientModel(token=os.getenv('HF_TOKEN'))\n",
    "agent = CodeAgent(tools=[*tools], model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7464c1ec",
   "metadata": {},
   "source": [
    "Now that we have the tools, we can create a simple agent that uses them to answer questions. We'll just use a simple `InferenceClientModel` and the default model from `smolagents` for now.\n",
    "\n",
    "It's important to pass your api key to the `InferenceClientModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93b4e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InferenceClientModel(token=os.getenv('HF_TOKEN'))\n",
    "agent = CodeAgent(tools=[*tools], model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c7185",
   "metadata": {},
   "source": [
    "Now, we can create a simple Gradio interface that uses the agent to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49dfd695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span> <span style=\"font-weight: bold\">Prime factorization of 68</span>                                                                                       <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÇ</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚ï∞‚îÄ InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚ï≠‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïÆ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m \u001b[1mPrime factorization of 68\u001b[0m                                                                                       \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚îÇ\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m‚îÇ\u001b[0m\n",
       "\u001b[38;2;212;183;2m‚ï∞‚îÄ\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen2.5-Coder-32B-Instruct \u001b[0m\u001b[38;2;212;183;2m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[38;2;212;183;2m‚îÄ‚ïØ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step 1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep 1\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ‚îÄ <span style=\"font-weight: bold\">Executing parsed code:</span> ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">prime_factors </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> mcp_tool_http_prime_factors(n</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"68\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                                                            </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(prime_factors)</span><span style=\"background-color: #272822\">                                                                                           </span>  \n",
       " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ‚îÄ \u001b[1mExecuting parsed code:\u001b[0m ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprime_factors\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmcp_tool_http_prime_factors\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mn\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m68\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                            \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprime_factors\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                                           \u001b[0m  \n",
       " ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "[2, 2, 17]\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "[2, 2, 17]\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 5.84 seconds| Input tokens: 2,196 | Output tokens: 73]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 5.84 seconds| Input tokens: 2,196 | Output tokens: 73]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ </span><span style=\"font-weight: bold\">Step 2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ \u001b[0m\u001b[1mStep 2\u001b[0m\u001b[38;2;212;183;2m ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn = lambda message, history: str(agent.run(message)),\n",
    "    type=\"messages\",\n",
    "    examples=[\"Prime factorization of 68\"],\n",
    "    title=\"Agent with MCP Tools\",\n",
    "    description=\"This is a simple agent that uses MCP tools to answer questions.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b167c988",
   "metadata": {},
   "source": [
    "And there ya go. Like I found out some time ago, huggingface is pretty easy to work with. Might take some time to get used to the `gradio` integration, but it's all part of the process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
