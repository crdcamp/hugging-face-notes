{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75175fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e4146b9",
   "metadata": {},
   "source": [
    "# Building Tiny Agents with MCP and the Hugging Face Hub\n",
    "\n",
    "Now that we've built MCP servers in Gradio and learned about creating MCP clients, let's complete out end-to-end application by building an agent that can seamlessly interact with our sentiment analysis tool. This section builds on the Tiny Agents, which demonstrates a super simple way of deploying MCP clients that can connect to services like our Gradio sentiment analysis server.\n",
    "\n",
    "In this final exercise of Unit 2, we will walk you through how to implement both TypeScript (JS) and Python MCP clients that can communicate with any MCP server, including the Gradio-based sentiment analysis server we built in the previous sections. This completes our end-to-end MCP application flow: from building a Gradio MCP server exposing a sentiment analysis tool, to creating a flexible agent that can use this tool alongside other capabilities.\n",
    "\n",
    "## Installation\n",
    "\n",
    "`npm install -g npx`\n",
    "\n",
    "`npm i mcp-remote`\n",
    "\n",
    "`pip install \"huggingface_hub[mcp]>=0.32.0\"`\n",
    "\n",
    "## Tiny Agents MCP Client in the Command Line\n",
    "\n",
    "Let’s setup a project with a basic Tiny Agent.\n",
    "\n",
    "```terminal\n",
    "mkdir my-agent\n",
    "touch my-agent/agent.json\n",
    "cd my-agent\n",
    "```\n",
    "\n",
    "We can then run the agent with the following command:\n",
    "\n",
    "`tiny-agents run agent.json`\n",
    "\n",
    "Here we have a basic Tiny Agent that can connect to our Gradio MCP server. It includes a model, provider, and a server configuration.\n",
    "\n",
    "We could also use an open source model running locally with Tiny Agents. If we start a local inference server with\n",
    "\n",
    "```javascript\n",
    "{\n",
    "\t\"model\": \"Qwen/Qwen3-32B\",\n",
    "\t\"endpointUrl\": \"http://localhost:1234/v1\",\n",
    "\t\"servers\": [\n",
    "\t\t{\n",
    "\t\t\t\"type\": \"stdio\",\n",
    "\t\t\t\"command\": \"npx\",\n",
    "\t\t\t\"args\": [\n",
    "\t\t\t\t\"mcp-remote\",\n",
    "\t\t\t\t\"http://localhost:1234/v1/mcp/sse\"\n",
    "\t\t\t]\n",
    "\t\t}\n",
    "\t]\n",
    "}\n",
    "```\n",
    "\n",
    "## Custom Tiny Agents MCP Client\n",
    "\n",
    "The beauty of MCP is that it provides a standardized way for agents to interact with any MCP-compatible server, including our Gradio-based sentiment analysis server from earlier sections.\n",
    "\n",
    "## Using the Gradio Server with Tiny Agents\n",
    "\n",
    "To connect our Tiny Agent to the Gradio sentiment analysis server we built earlier in this unit, we just need to add it to our list of servers. Here’s how we can modify our agent configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb34f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "    provider=\"nebius\",\n",
    "    servers=[\n",
    "        {\n",
    "            \"command\": \"npx\",\n",
    "            \"args\": [\n",
    "                \"mcp-remote\",\n",
    "                \"http://localhost:7860/gradio_api/mcp/sse\"  # Your Gradio MCP server\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f84948",
   "metadata": {},
   "source": [
    "Now out agent can use the sentiment analysis tool alongside other tools. For example, it could:\n",
    "1. Read text from a file using the filesystem server\n",
    "1. Analyze its sentiment using our Gradio server\n",
    "1. Write the results back to a file\n",
    "\n",
    "# Deployment Considerations\n",
    "\n",
    "When deploying your Gradio MCP server to Hugging Face Spaces, you’ll need to update the server URL in your agent configuration to point to your deployed space:\n",
    "\n",
    "```javascript\n",
    "{\n",
    "    command: \"npx\",\n",
    "    args: [\n",
    "        \"mcp-remote\",\n",
    "        \"https://YOUR_USERNAME-mcp-sentiment.hf.space/gradio_api/mcp/sse\"\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This allows your agent to use the sentiment analysis tool from anywhere, not just locally!\n",
    "\n",
    "Next up is [Local Tiny Agents with AMD NPU and iGPU Acceleration](https://huggingface.co/learn/mcp-course/en/unit2/lemonade-server), which sounds a lot like something I don't need to know. Let's skip that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
